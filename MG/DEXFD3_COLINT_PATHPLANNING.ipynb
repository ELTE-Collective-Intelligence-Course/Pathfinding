{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzioeAra8WgC",
        "outputId": "8f047a9c-28ef-4399-d727-1a6571f7452a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/953.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m839.7/953.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting supersuit\n",
            "  Downloading SuperSuit-3.9.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n",
            "Collecting tinyscaler>=1.2.6 (from supersuit)\n",
            "  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n",
            "Installing collected packages: tinyscaler, supersuit\n",
            "Successfully installed supersuit-3.9.1 tinyscaler-1.2.7\n",
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.24.2-py3-none-any.whl (846 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.8/846.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (1.23.5)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n",
            "Installing collected packages: pettingzoo\n",
            "Successfully installed pettingzoo-1.24.2\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.2.1\n",
            "Collecting sb3_contrib\n",
            "  Downloading sb3_contrib-2.2.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: stable-baselines3<3.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from sb3_contrib) (2.2.1)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.2.1->sb3_contrib) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.2.1->sb3_contrib) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3<3.0,>=2.2.1->sb3_contrib) (1.3.0)\n",
            "Installing collected packages: sb3_contrib\n",
            "Successfully installed sb3_contrib-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install supersuit\n",
        "!pip install pettingzoo\n",
        "!pip install stable_baselines3\n",
        "!pip install sb3_contrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLUtmylCT9Mp",
        "outputId": "c8596da6-58d4-4d1d-aa51-2d5873e618ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#from pettingzoo.mpe import simple_spread_v3\n",
        "from pprint import pprint\n",
        "import math\n",
        "from enum import Enum\n",
        "import random\n",
        "import pygame\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch as th\n",
        "\n",
        "from supersuit import pettingzoo_env_to_vec_env_v1, concat_vec_envs_v1\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "from stable_baselines3.common.callbacks import ProgressBarCallback\n",
        "\n",
        "from sb3_contrib import RecurrentPPO\n",
        "# from stable_baselines3.common.evaluation import evaluate_policy\n",
        "# from stable_baselines3 import DQN\n",
        "# from stable_baselines3.dqn import CnnPolicy, MlpPolicy\n",
        "\n",
        "\n",
        "import random\n",
        "import pygame\n",
        "import numpy as np\n",
        "from gymnasium.utils import EzPickle\n",
        "\n",
        "from pettingzoo.mpe._mpe_utils.core import Agent, Landmark, World\n",
        "from pettingzoo.mpe._mpe_utils.scenario import BaseScenario\n",
        "from pettingzoo.mpe._mpe_utils.simple_env import SimpleEnv, make_env\n",
        "from pettingzoo.utils.conversions import parallel_wrapper_fn\n",
        "\n",
        "\n",
        "from typing import Callable\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT5Y3-10T92T"
      },
      "outputs": [],
      "source": [
        "class CustomWorld(World):\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.dt = 0.05\n",
        "\t\tself.contact_force = 200\n",
        "\n",
        "\t@property\n",
        "\tdef entities(self):\n",
        "\t\t### Modification ###\n",
        "\t\t# add obstacles to entities\n",
        "\t\tentities = self.agents + self.landmarks + self.obstacles\n",
        "\t\treturn entities\n",
        "\t\t####################\n",
        "\n",
        "\t# gather agent action forces\n",
        "\tdef apply_action_force(self, p_force):\n",
        "\t\t# set applied forces\n",
        "\t\tfor i, agent in enumerate(self.agents):\n",
        "\t\t\tif agent.movable:\n",
        "\t\t\t\tnoise = (\n",
        "\t\t\t\t\tnp.random.randn(*agent.action.u.shape) * agent.u_noise\n",
        "\t\t\t\t\tif agent.u_noise\n",
        "\t\t\t\t\telse 0.0\n",
        "\t\t\t\t)\n",
        "\t\t\t\tp_force[i] = agent.action.u + noise\n",
        "\t\treturn p_force\n",
        "\n",
        "\t# integrate physical state\n",
        "\tdef integrate_state(self, p_force):\n",
        "\t\tfor i, entity in enumerate(self.entities):\n",
        "\t\t\tif not entity.movable:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t### Modification ###\n",
        "\t\t\t# don't let entity go out of the map\n",
        "\n",
        "\t\t\tnew_entity_pos = entity.state.p_pos + entity.state.p_vel * self.dt\n",
        "\t\t\tentity.state.last_p_vel = entity.state.p_vel\n",
        "\t\t\tif (new_entity_pos >= -1).all() and (new_entity_pos <= 1).all():\n",
        "\t\t\t\tentity.state.p_pos = new_entity_pos\n",
        "\t\t\t####################\n",
        "\n",
        "\t\t\tentity.state.p_vel = entity.state.p_vel * (1 - self.damping)\n",
        "\t\t\tif p_force[i] is not None:\n",
        "\t\t\t\tentity.state.p_vel += (p_force[i] / entity.mass) * self.dt\n",
        "\t\t\tif entity.max_speed is not None:\n",
        "\t\t\t\tspeed = np.sqrt(\n",
        "\t\t\t\t\tnp.square(entity.state.p_vel[0]) + np.square(entity.state.p_vel[1])\n",
        "\t\t\t\t)\n",
        "\t\t\t\tif speed > entity.max_speed:\n",
        "\t\t\t\t\tentity.state.p_vel = (\n",
        "\t\t\t\t\t\tentity.state.p_vel\n",
        "\t\t\t\t\t\t/ np.sqrt(\n",
        "\t\t\t\t\t\t\tnp.square(entity.state.p_vel[0])\n",
        "\t\t\t\t\t\t\t+ np.square(entity.state.p_vel[1])\n",
        "\t\t\t\t\t\t)\n",
        "\t\t\t\t\t\t* entity.max_speed\n",
        "\t\t\t\t\t)\n",
        "\n",
        "\n",
        "class CustomSimpleEnv(SimpleEnv):\n",
        "\tdef draw(self):\n",
        "\t\t# clear screen\n",
        "\t\tself.screen.fill((255, 255, 255))\n",
        "\n",
        "\t\t### Modification ###\n",
        "\t\t# draw border\n",
        "\t\tpygame.draw.rect(self.screen, (0, 0, 0, 0), rect=pygame.Rect(0, 0, self.width, self.height), width=25)\n",
        "\t\t####################\n",
        "\n",
        "\t\t# update bounds to center around agent\n",
        "\t\tall_poses = [entity.state.p_pos for entity in self.world.entities]\n",
        "\t\tcam_range = 1\n",
        "\n",
        "\t\t# update geometry and text positions\n",
        "\t\ttext_line = 0\n",
        "\t\tfor e, entity in enumerate(self.world.entities):\n",
        "\t\t\t# geometry\n",
        "\t\t\tx, y = entity.state.p_pos\n",
        "\t\t\ty *= (\n",
        "\t\t\t\t-1\n",
        "\t\t\t)  # this makes the display mimic the old pyglet setup (ie. flips image)\n",
        "\t\t\tx = (\n",
        "\t\t\t\t(x / cam_range) * self.width // 2 * 0.9\n",
        "\t\t\t)  # the .9 is just to keep entities from appearing \"too\" out-of-bounds\n",
        "\t\t\ty = (y / cam_range) * self.height // 2 * 0.9\n",
        "\t\t\tx += self.width // 2\n",
        "\t\t\ty += self.height // 2\n",
        "\t\t\tpygame.draw.circle(\n",
        "\t\t\t\tself.screen, entity.color * 200, (x, y), entity.size * 350\n",
        "\t\t\t)  # 350 is an arbitrary scale factor to get pygame to render similar sizes as pyglet\n",
        "\t\t\tpygame.draw.circle(\n",
        "\t\t\t\tself.screen, (0, 0, 0), (x, y), entity.size * 350, 1\n",
        "\t\t\t)  # borders\n",
        "\t\t\tassert (\n",
        "\t\t\t\t0 < x < self.width and 0 < y < self.height\n",
        "\t\t\t), f\"Coordinates {(x, y)} are out of bounds.\"\n",
        "\n",
        "\t\t\t# text\n",
        "\t\t\tif isinstance(entity, Agent):\n",
        "\t\t\t\tif entity.silent:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tif np.all(entity.state.c == 0):\n",
        "\t\t\t\t\tword = \"_\"\n",
        "\t\t\t\telif self.continuous_actions:\n",
        "\t\t\t\t\tword = (\n",
        "\t\t\t\t\t\t\"[\" + \",\".join([f\"{comm:.2f}\" for comm in entity.state.c]) + \"]\"\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tword = alphabet[np.argmax(entity.state.c)]\n",
        "\n",
        "\t\t\t\tmessage = entity.name + \" sends \" + word + \"   \"\n",
        "\t\t\t\tmessage_x_pos = self.width * 0.05\n",
        "\t\t\t\tmessage_y_pos = self.height * 0.95 - (self.height * 0.05 * text_line)\n",
        "\t\t\t\tself.game_font.render_to(\n",
        "\t\t\t\t\tself.screen, (message_x_pos, message_y_pos), message, (0, 0, 0)\n",
        "\t\t\t\t)\n",
        "\t\t\t\ttext_line += 1\n",
        "\n",
        "\tdef reset(self, seed=None, options=None):\n",
        "\t\tif seed is not None:\n",
        "\t\t\tself._seed(seed=seed)\n",
        "\t\tself.scenario.reset_world(self.world, self.np_random)\n",
        "\n",
        "\t\tself.agents = self.possible_agents[:]\n",
        "\t\tself.rewards = {name: 0.0 for name in self.agents}\n",
        "\t\tself._cumulative_rewards = {name: 0.0 for name in self.agents}\n",
        "\t\tself.terminations = {name: False for name in self.agents}\n",
        "\t\tself.truncations = {name: False for name in self.agents}\n",
        "\t\tself.infos = {name: {} for name in self.agents}\n",
        "\n",
        "\t\tself.agent_selection = self._agent_selector.reset()\n",
        "\t\tself.steps = 0\n",
        "\n",
        "\t\tself.current_actions = [None] * self.num_agents\n",
        "\n",
        "\t\tif False:\n",
        "\t\t\tfor agent in self.world.agents:\n",
        "\t\t\t\tself._cumulative_rewards[agent.name] = self.scenario.reward(agent, self.world)\n",
        "\n",
        "\n",
        "class raw_env(CustomSimpleEnv, EzPickle):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tN=3,\n",
        "\t\tO=5,\n",
        "\t\tlocal_ratio=0.5,\n",
        "\t\tmax_cycles=25,\n",
        "\t\tcontinuous_actions=False,\n",
        "\t\trender_mode=None,\n",
        "\t):\n",
        "\t\tEzPickle.__init__(\n",
        "\t\t\tself,\n",
        "\t\t\tN=N,\n",
        "\t\t\tO=O,\n",
        "\t\t\tlocal_ratio=local_ratio,\n",
        "\t\t\tmax_cycles=max_cycles,\n",
        "\t\t\tcontinuous_actions=continuous_actions,\n",
        "\t\t\trender_mode=render_mode,\n",
        "\t\t)\n",
        "\t\tassert (\n",
        "\t\t\t0.0 <= local_ratio <= 1.0\n",
        "\t\t), \"local_ratio is a proportion. Must be between 0 and 1.\"\n",
        "\t\tscenario = PathFindingScenario()\n",
        "\t\tworld = scenario.make_world(N, O)\n",
        "\t\tCustomSimpleEnv.__init__(\n",
        "\t\t\tself,\n",
        "\t\t\tscenario=scenario,\n",
        "\t\t\tworld=world,\n",
        "\t\t\trender_mode=render_mode,\n",
        "\t\t\tmax_cycles=max_cycles,\n",
        "\t\t\tcontinuous_actions=continuous_actions,\n",
        "\t\t\tlocal_ratio=local_ratio,\n",
        "\t\t)\n",
        "\t\tself.metadata[\"name\"] = \"simple_spread_v3_modified\"\n",
        "\t\tself.scenario = scenario\n",
        "\t\tself.world = world\n",
        "\n",
        "\n",
        "custom_env = make_env(raw_env)\n",
        "\n",
        "\n",
        "class PathFindingScenario(BaseScenario):\n",
        "\tdef make_world(self, N=3, O=5, agent_size=0.025, landmark_size = 0.1, obstacle_size = 0.06):\n",
        "\t\tworld = CustomWorld()\n",
        "\t\t# set any world properties first\n",
        "\t\tworld.dim_c = 2\n",
        "\t\tworld.num_agents = N\n",
        "\t\tworld.num_landmarks = N\n",
        "\t\tworld.num_obstacles = O\n",
        "\t\tworld.collaborative = True\n",
        "\n",
        "\t\tworld.agent_size = agent_size\n",
        "\t\tworld.landmark_size = landmark_size\n",
        "\t\tworld.obstacle_size = obstacle_size\n",
        "\n",
        "\t\t# add agents\n",
        "\t\tworld.agents = [Agent() for i in range(world.num_agents)]\n",
        "\t\tfor i, agent in enumerate(world.agents):\n",
        "\t\t\tagent.name = f\"agent_{i}\"\n",
        "\t\t\tagent.agent_id = i\n",
        "\t\t\tagent.collide = True\n",
        "\t\t\tagent.silent = True\n",
        "\t\t\tagent.size = world.agent_size\n",
        "\n",
        "\t\t# add landmarks\n",
        "\t\tworld.landmarks = [Landmark() for i in range(world.num_landmarks)]\n",
        "\t\tfor i, landmark in enumerate(world.landmarks):\n",
        "\t\t\tlandmark.name = f\"landmark_{i}\"\n",
        "\t\t\tlandmark.landmark_id = i\n",
        "\t\t\tlandmark.collide = False\n",
        "\t\t\tlandmark.movable = False\n",
        "\t\t\tlandmark.size = world.landmark_size\n",
        "\n",
        "\t\t# add obstacles\n",
        "\t\tworld.obstacles\t= [Landmark() for i in range(world.num_obstacles)]\n",
        "\t\tfor i, obstacle in enumerate(world.obstacles):\n",
        "\t\t\tobstacle.name = f\"obstacle_{i}\"\n",
        "\t\t\tobstacle.obstacle_id = i\n",
        "\t\t\tobstacle.collide = True\n",
        "\t\t\tobstacle.movable = False\n",
        "\t\t\tobstacle.size = world.obstacle_size\n",
        "\n",
        "\t\treturn world\n",
        "\n",
        "\tdef _place_entity_on_border(self, margin = 0):\n",
        "\t\tr = random.randint(0, 3)\n",
        "\t\tminimum = -1 + margin * random.random()\n",
        "\t\tmaximum = 1 - margin * random.random()\n",
        "\t\tif r == 0:\n",
        "\t\t\tx = random.random() * 2 - 1\n",
        "\t\t\ty = minimum\n",
        "\t\telif r == 1:\n",
        "\t\t\tx = random.random() * 2 - 1\n",
        "\t\t\ty = maximum\n",
        "\t\telif r == 2:\n",
        "\t\t\tx = minimum\n",
        "\t\t\ty = random.random() * 2 - 1\n",
        "\t\telse:\n",
        "\t\t\tx = maximum\n",
        "\t\t\ty = random.random() * 2 - 1\n",
        "\t\treturn np.array([x, y])\n",
        "\n",
        "\tdef get_color(self, i, N, axis):\n",
        "\t\tcolor_value_base = 0.5\n",
        "\t\tcolor_value_max = 0.9\n",
        "\t\tremaining_range = color_value_max - color_value_base\n",
        "\t\tstep = remaining_range / ((N-1) if N > 1 else 1)\n",
        "\n",
        "\t\tcolor_value = color_value_base + step * i\n",
        "\t\tcolor = np.zeros((3,))\n",
        "\t\tcolor[axis] = color_value\n",
        "\t\treturn color\n",
        "\n",
        "\tdef reset_world(self, world, np_random):\n",
        "\t\t# random properties for agents\n",
        "\t\tfor i, agent in enumerate(world.agents):\n",
        "\t\t\tagent.color = self.get_color(i, world.num_agents, 2)\n",
        "\n",
        "\t\t# random properties for landmarks\n",
        "\t\tfor i, landmark in enumerate(world.landmarks):\n",
        "\t\t\tlandmark.color = self.get_color(i, world.num_landmarks, 0)\n",
        "\n",
        "\t\t# random properties for obstacles\n",
        "\t\tfor i, obstacle in enumerate(world.obstacles):\n",
        "\t\t\tobstacle.color = np.array([0.30, 0.15, 0.15])\n",
        "\n",
        "\t\tpositions = set()\n",
        "\n",
        "\t\t# set random initial states\n",
        "\t\tfor i, landmark in enumerate(world.landmarks):\n",
        "\t\t\tlandmark.state.p_pos = self._place_entity_on_border(landmark.size * 3)  # place landmark on the border\n",
        "\t\t\tlandmark.state.p_vel = np.zeros(world.dim_p)\n",
        "\t\t\tlandmark.state.last_p_vel = np.zeros(world.dim_p)\n",
        "\t\t\tpositions.add(tuple(landmark.state.p_pos))\n",
        "\t\tfor i, agent in enumerate(world.agents):\n",
        "\t\t\tagent.state.p_pos = world.landmarks[i].state.p_pos * -1  # place agent in the opposite side of the map as its target\n",
        "\t\t\tagent.state.p_vel = np.zeros(world.dim_p)\n",
        "\t\t\tagent.state.c = np.zeros(world.dim_c)\n",
        "\t\t\tagent.state.last_p_vel = np.zeros(world.dim_p)\n",
        "\t\tfor i, obstacle in enumerate(world.obstacles):\n",
        "\t\t\tobstacle.state.p_pos = np_random.uniform(\n",
        "\t\t\t\t-1 + world.obstacle_size * 1.5, +1 - world.obstacle_size * 1.5, world.dim_p)  # place obtsacle randomly inside the map, but not on the border\n",
        "\t\t\tobstacle.state.p_vel = np.zeros(world.dim_p)\n",
        "\t\t\tpositions.add(tuple(obstacle.state.p_pos))\n",
        "\t\t\tobstacle.state.last_p_vel = np.zeros(world.dim_p)\n",
        "\n",
        "\t\t# set collision counters to 0\n",
        "\t\tworld.collision_counter = {}\n",
        "\t\tfor a in world.agents:\n",
        "\t\t\tworld.collision_counter[a.name] = {\n",
        "\t\t\t\t\"agent\": 0,\n",
        "\t\t\t\t\"obstacle\": 0,\n",
        "\t\t\t\t\"border\": 0\n",
        "\t\t\t}\n",
        "\t\tworld.distance_from_target = {}\n",
        "\t\tfor a in world.agents:\n",
        "\t\t\tworld.distance_from_target[a.name] = 0\n",
        "\n",
        "\tdef is_collision(self, agent1, agent2):\n",
        "\t\tdelta_pos = agent1.state.p_pos - agent2.state.p_pos\n",
        "\t\tdist = np.sqrt(np.sum(np.square(delta_pos)))\n",
        "\t\tdist_min = agent1.size + agent2.size\n",
        "\t\treturn True if (dist < dist_min) else False\n",
        "\n",
        "\tdef get_distance_l2(self, pos1, pos2):\n",
        "\t\treturn np.sqrt(np.sum(np.square(pos1 - pos2)))\n",
        "\n",
        "\tdef get_distance_l1(self, pos1, pos2):\n",
        "\t\treturn np.abs(np.sum(np.square(pos1 - pos2)))\n",
        "\n",
        "\tdef get_distance_reward_log(self, distance):\n",
        "\t\tif distance <= 0:\n",
        "\t\t\treturn -250\n",
        "\n",
        "\t\treturn (np.log(distance) - 1)\n",
        "\n",
        "\tdef get_distance_reward_square(self, distance):\n",
        "\t\tif distance <= 0:\n",
        "\t\t\treturn 0\n",
        "\n",
        "\t\treturn distance ** 2\n",
        "\n",
        "\tdef reward(self, agent, world):\n",
        "\t\t# Agents are penalized for collisions and for being on the border\n",
        "\t\trew = 0\n",
        "\t\tcollisions_agent = 0\n",
        "\t\tcollisions_obstacle = 0\n",
        "\t\tcollisions_border = 0\n",
        "\n",
        "\t\treward_per_collision = -5\n",
        "\n",
        "\t\tif agent.collide:\n",
        "\t\t\tfor a in world.agents:\n",
        "\t\t\t\tis_collision = (self.is_collision(a, agent) and a != agent)\n",
        "\t\t\t\tif is_collision:\n",
        "\t\t\t\t\tcollisions_agent += 1\n",
        "\t\t\t\t\tif True:\n",
        "\t\t\t\t\t\trew += reward_per_collision\n",
        "\t\t\tfor o in world.obstacles:\n",
        "\t\t\t\tis_collision = (self.is_collision(o, agent))\n",
        "\t\t\t\tif is_collision:\n",
        "\t\t\t\t\tcollisions_obstacle += 1\n",
        "\t\t\t\t\tif True:\n",
        "\t\t\t\t\t\trew += reward_per_collision\n",
        "\t\tif agent.state.p_pos[0] in (-1, 1) or agent.state.p_pos[1] in (-1 , 1):\n",
        "\t\t\tcollisions_border += 1\n",
        "\t\t\tif False:\n",
        "\t\t\t\trew += reward_per_collision\n",
        "\n",
        "\t\ttarget = world.landmarks[agent.agent_id]\n",
        "\t\tdistance = self.get_distance_l2(agent.state.p_pos, target.state.p_pos) - agent.size - target.size\n",
        "\n",
        "\t\tr = self.get_distance_reward_log(distance)\n",
        "\n",
        "\t\trew -= r\n",
        "\n",
        "\t\tworld.collision_counter[agent.name][\"agent\"] = collisions_agent\n",
        "\t\tworld.collision_counter[agent.name][\"obstacle\"] = collisions_obstacle\n",
        "\t\tworld.collision_counter[agent.name][\"border\"] = collisions_border\n",
        "\n",
        "\t\tworld.distance_from_target[agent.name] = distance\n",
        "\n",
        "\t\treturn rew\n",
        "\n",
        "\tdef global_reward(self, world):\n",
        "\t\t# Reward based on how close the agent is to it's target\n",
        "\t\trew = 0\n",
        "\t\t# TODO: change back\n",
        "\t\t# for i, agent in enumerate(world.agents):\n",
        "\t\t#\ttarget = world.landmarks[i]\n",
        "\t\t#\tdist = np.sqrt(np.sum(np.square(agent.state.p_pos - target.state.p_pos)))\n",
        "\t\t#\trew -= dist\n",
        "\t\treturn rew\n",
        "\n",
        "\tdef observation(self, agent, world):\n",
        "\t\t# get positions of all entities in this agent's reference frame\n",
        "\n",
        "\t\tentity = world.landmarks[agent.agent_id]\n",
        "\t\tentity_pos = entity.state.p_pos - agent.state.p_pos\n",
        "\n",
        "\t\tif self.is_collision(entity, agent):\n",
        "\t\t\tentity_pos = np.array([0, 0])\n",
        "\n",
        "\t\tif False:\n",
        "\t\t\tif self.is_collision(entity, agent):\n",
        "\t\t\t\tentity_pos = np.array([0, 0])\n",
        "\t\t\telse:\n",
        "\t\t\t\tif entity_pos[0] < 0:\n",
        "\t\t\t\t\tentity_pos[0] += agent.size\n",
        "\t\t\t\t\tentity_pos[0] += entity.size\n",
        "\t\t\t\t\tif entity_pos[0] > 0:\n",
        "\t\t\t\t\t\tentity_pos[0] = 0\n",
        "\t\t\t\telif entity_pos[0] > 0:\n",
        "\t\t\t\t\tentity_pos[0] -= agent.size\n",
        "\t\t\t\t\tentity_pos[0] -= entity.size\n",
        "\t\t\t\t\tif entity_pos[0] < 0:\n",
        "\t\t\t\t\t\tentity_pos[0] = 0\n",
        "\n",
        "\t\t\t\tif entity_pos[1] < 0:\n",
        "\t\t\t\t\tentity_pos[1] += agent.size\n",
        "\t\t\t\t\tentity_pos[1] += entity.size\n",
        "\t\t\t\t\tif entity_pos[1] > 0:\n",
        "\t\t\t\t\t\tentity_pos[1] = 0\n",
        "\t\t\t\telif entity_pos[1] > 0:\n",
        "\t\t\t\t\tentity_pos[1] -= agent.size\n",
        "\t\t\t\t\tentity_pos[1] -= entity.size\n",
        "\t\t\t\t\tif entity_pos[1] < 0:\n",
        "\t\t\t\t\t\tentity_pos[1] = 0\n",
        "\n",
        "\n",
        "\t\t# communication of all other agents\n",
        "\t\tentities = []\n",
        "\t\tfor other in world.agents:\n",
        "\t\t\tif other is agent:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tnew_other_pos = other.state.p_pos + other.state.last_p_vel * 0.05\n",
        "\t\t\tdistance = self.get_distance_l2(other.state.p_pos, agent.state.p_pos)\n",
        "\t\t\tdistance_from_edges = distance - agent.size - other.size\n",
        "\n",
        "\t\t\t# pos\n",
        "\t\t\trelative_position = other.state.p_pos - agent.state.p_pos\n",
        "\n",
        "\t\t\trelative_position_0 = relative_position[0]\n",
        "\t\t\trelative_position_1 = relative_position[1]\n",
        "\t\t\tif self.is_collision(other, agent):\n",
        "\t\t\t\trelative_position_0 = 0\n",
        "\t\t\t\trelative_position_1 = 0\n",
        "\t\t\telse:\n",
        "\t\t\t\tif relative_position_0 < 0:\n",
        "\t\t\t\t\trelative_position_0 += agent.size\n",
        "\t\t\t\t\trelative_position_0 += other.size\n",
        "\t\t\t\t\tif relative_position_0 > 0:\n",
        "\t\t\t\t\t\trelative_position_0 = 0\n",
        "\t\t\t\telif relative_position_0 > 0:\n",
        "\t\t\t\t\trelative_position_0 -= agent.size\n",
        "\t\t\t\t\trelative_position_0 -= other.size\n",
        "\t\t\t\t\tif relative_position_0 < 0:\n",
        "\t\t\t\t\t\trelative_position_0 = 0\n",
        "\n",
        "\t\t\t\tif relative_position_1 < 0:\n",
        "\t\t\t\t\trelative_position_1 += agent.size\n",
        "\t\t\t\t\trelative_position_1 += other.size\n",
        "\t\t\t\t\tif relative_position_1 > 0:\n",
        "\t\t\t\t\t\trelative_position_1 = 0\n",
        "\t\t\t\telif relative_position_1 > 0:\n",
        "\t\t\t\t\trelative_position_1 -= agent.size\n",
        "\t\t\t\t\trelative_position_1 -= other.size\n",
        "\t\t\t\t\tif relative_position_1 < 0:\n",
        "\t\t\t\t\t\trelative_position_1 = 0\n",
        "\n",
        "\t\t\trelative_position_from_edges = np.array([relative_position_0, relative_position_1])\n",
        "\n",
        "\n",
        "\t\t\t# new pos\n",
        "\t\t\trelative_position = new_other_pos - agent.state.p_pos\n",
        "\n",
        "\t\t\trelative_position_0 = relative_position[0]\n",
        "\t\t\trelative_position_1 = relative_position[1]\n",
        "\t\t\tif self.is_collision(other, agent):\n",
        "\t\t\t\trelative_position_0 = 0\n",
        "\t\t\t\trelative_position_1 = 0\n",
        "\t\t\telse:\n",
        "\t\t\t\tif relative_position_0 < 0:\n",
        "\t\t\t\t\trelative_position_0 += agent.size\n",
        "\t\t\t\t\trelative_position_0 += other.size\n",
        "\t\t\t\t\tif relative_position_0 > 0:\n",
        "\t\t\t\t\t\trelative_position_0 = 0\n",
        "\t\t\t\telif relative_position_0 > 0:\n",
        "\t\t\t\t\trelative_position_0 -= agent.size\n",
        "\t\t\t\t\trelative_position_0 -= other.size\n",
        "\t\t\t\t\tif relative_position_0 < 0:\n",
        "\t\t\t\t\t\trelative_position_0 = 0\n",
        "\n",
        "\t\t\t\tif relative_position_1 < 0:\n",
        "\t\t\t\t\trelative_position_1 += agent.size\n",
        "\t\t\t\t\trelative_position_1 += other.size\n",
        "\t\t\t\t\tif relative_position_1 > 0:\n",
        "\t\t\t\t\t\trelative_position_1 = 0\n",
        "\t\t\t\telif relative_position_1 > 0:\n",
        "\t\t\t\t\trelative_position_1 -= agent.size\n",
        "\t\t\t\t\trelative_position_1 -= other.size\n",
        "\t\t\t\t\tif relative_position_1 < 0:\n",
        "\t\t\t\t\t\trelative_position_1 = 0\n",
        "\n",
        "\t\t\tnew_relative_position_from_edges = np.array([relative_position_0, relative_position_1])\n",
        "\n",
        "\t\t\tentities.append((relative_position_from_edges, distance_from_edges, new_relative_position_from_edges))\n",
        "\n",
        "\t\tfor obstacle in world.obstacles:\n",
        "\t\t\tdistance = self.get_distance_l2(obstacle.state.p_pos, agent.state.p_pos)\n",
        "\t\t\tdistance_from_edges = distance - agent.size - obstacle.size\n",
        "\n",
        "\t\t\trelative_position = obstacle.state.p_pos - agent.state.p_pos\n",
        "\n",
        "\t\t\trelative_position_0 = relative_position[0]\n",
        "\t\t\trelative_position_1 = relative_position[1]\n",
        "\t\t\tif self.is_collision(obstacle, agent):\n",
        "\t\t\t\trelative_position_0 = 0\n",
        "\t\t\t\trelative_position_1 = 0\n",
        "\t\t\telse:\n",
        "\t\t\t\tif relative_position_0 < 0:\n",
        "\t\t\t\t\trelative_position_0 += agent.size\n",
        "\t\t\t\t\trelative_position_0 += obstacle.size\n",
        "\t\t\t\t\tif relative_position_0 > 0:\n",
        "\t\t\t\t\t\trelative_position_0 = 0\n",
        "\t\t\t\telif relative_position_0 > 0:\n",
        "\t\t\t\t\trelative_position_0 -= agent.size\n",
        "\t\t\t\t\trelative_position_0 -= obstacle.size\n",
        "\t\t\t\t\tif relative_position_0 < 0:\n",
        "\t\t\t\t\t\trelative_position_0 = 0\n",
        "\n",
        "\t\t\t\tif relative_position_1 < 0:\n",
        "\t\t\t\t\trelative_position_1 += agent.size\n",
        "\t\t\t\t\trelative_position_1 += obstacle.size\n",
        "\t\t\t\t\tif relative_position_1 > 0:\n",
        "\t\t\t\t\t\trelative_position_1 = 0\n",
        "\t\t\t\telif relative_position_1 > 0:\n",
        "\t\t\t\t\trelative_position_1 -= agent.size\n",
        "\t\t\t\t\trelative_position_1 -= obstacle.size\n",
        "\t\t\t\t\tif relative_position_1 < 0:\n",
        "\t\t\t\t\t\trelative_position_1 = 0\n",
        "\n",
        "\t\t\trelative_position_from_edges = np.array([relative_position_0, relative_position_1])\n",
        "\n",
        "\t\t\tentities.append((relative_position_from_edges, distance_from_edges, relative_position_from_edges))\n",
        "\n",
        "\t\tentities.sort(key=lambda x: x[1])\n",
        "\n",
        "\t\tentities_new_pos = [p[2] for p in entities]\n",
        "\t\tentities = [p[0] for p in entities]\n",
        "\n",
        "\t\tentities = entities[0]\n",
        "\t\tentities_new_pos = entities_new_pos[0]\n",
        "\n",
        "\t\tobs = np.concatenate(\n",
        "\t\t\t[entity_pos] + [entities] + [entities_new_pos]\n",
        "\t\t).astype(float)\n",
        "\n",
        "\t\tdivide_by = np.max(np.abs(obs)).item()\n",
        "\t\tif divide_by:\n",
        "\t\t\tobs /= divide_by\n",
        "\n",
        "\t\treturn obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6RGoXJXUAxD"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "\n",
        "\tdef __init__(self, name: str, env_fn: Callable, seed: float, cpu_count: int, n_steps: int, n_epochs: int, progress_bar: bool, batch_size: int, train_settings: dict, eval_settings: dict, learning_rate: float = 0.0001, ent_coef: float = 0.001):\n",
        "\t\t'''\n",
        "\t\t\tCreated environment and model\n",
        "\t\t'''\n",
        "\t\tprint(\"Creating model for training\")\n",
        "\n",
        "\t\tself.name = name\n",
        "\t\tself.env_fn = env_fn\n",
        "\t\tself.seed = seed\n",
        "\t\tself.cpu_count = cpu_count\n",
        "\t\tself.n_steps = n_steps\n",
        "\t\tself.n_epochs = n_epochs\n",
        "\t\tself.progress_bar = progress_bar\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.ent_coef = ent_coef\n",
        "\n",
        "\t\tself.train_settings = train_settings\n",
        "\t\tself.eval_settings = eval_settings\n",
        "\n",
        "\t\tself.env = ModelTrainer.create_env(parallel_wrapper_fn(self.env_fn), self.seed, self.cpu_count, train_settings)\n",
        "\n",
        "\t\tself.model = ModelTrainer.create_model(self.name, self.env, self.n_steps, self.n_epochs, self.batch_size, self.learning_rate, self.ent_coef)\n",
        "\n",
        "\t\tself.trained_for = 0\n",
        "\n",
        "\t\tself.avg_rewards = {}\n",
        "\t\tself.avg_collisions = {\n",
        "\t\t\t\"agent\": {},\n",
        "\t\t\t\"obstacle\": {},\n",
        "\t\t\t\"border\": {}\n",
        "\t\t}\n",
        "\t\tself.avg_distance_from_target = {}\n",
        "\t\tself.avg_on_target = {}\n",
        "\n",
        "\t\tself.rewards = {}\n",
        "\t\tself.collisions = {\n",
        "\t\t\t\"agent\": {},\n",
        "\t\t\t\"obstacle\": {},\n",
        "\t\t\t\"border\": {}\n",
        "\t\t}\n",
        "\t\tself.distance_from_target = {}\n",
        "\t\tself.on_target = {}\n",
        "\n",
        "\tdef learn(self, update_count: int = 4, eval_after: int = 2):\n",
        "\t\tassert update_count >= eval_after, \"Not enough updates to reach eval\"\n",
        "\t\tassert (update_count/eval_after).is_integer(), \"Must be the same number of updates before every eval\"\n",
        "\n",
        "\t\tif self.trained_for == 0:\n",
        "\t\t\tmax_reward, avg_reward, avg_agent_collision, avg_obstacle_collision, avg_border_collision, avg_distance_from_target, avg_on_target, rewards, agent_collisions, obstacle_collisions, border_collisions, distance_from_target, on_target = self.eval(None)\n",
        "\t\t\tind = self.trained_for\n",
        "\t\t\tself.avg_rewards[ind] = avg_reward\n",
        "\t\t\tself.avg_collisions[\"agent\"][ind] = avg_agent_collision\n",
        "\t\t\tself.avg_collisions[\"obstacle\"][ind] = avg_obstacle_collision\n",
        "\t\t\tself.avg_collisions[\"border\"][ind] = avg_border_collision\n",
        "\t\t\tself.avg_distance_from_target[ind] = avg_distance_from_target\n",
        "\t\t\tself.avg_on_target[ind] = avg_on_target\n",
        "\n",
        "\t\t\tself.rewards[ind] = rewards\n",
        "\t\t\tself.collisions[\"agent\"][ind] = agent_collisions\n",
        "\t\t\tself.collisions[\"obstacle\"][ind] = obstacle_collisions\n",
        "\t\t\tself.collisions[\"border\"][ind] = border_collisions\n",
        "\t\t\tself.distance_from_target[ind] = distance_from_target\n",
        "\t\t\tself.on_target[ind] = on_target\n",
        "\t\telse:\n",
        "\t\t\tmax_reward, avg_reward, avg_agent_collision, avg_obstacle_collision, avg_border_collision, avg_distance_from_target, avg_on_target, rewards, agent_collisions, obstacle_collisions, border_collisions, distance_from_target, on_target = None, None, None, None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "\t\tagent_count = self.train_settings[\"N\"]\n",
        "\t\tenv_count = self.train_settings[\"env_count\"]\n",
        "\t\ttimesteps_per_update = agent_count * self.n_steps * env_count\n",
        "\t\ti = 0\n",
        "\t\twhile i < update_count:\n",
        "\t\t\tself.model.learn(total_timesteps=(timesteps_per_update * eval_after), progress_bar=self.progress_bar)\n",
        "\t\t\ti += eval_after\n",
        "\t\t\tind = self.trained_for + i\n",
        "\t\t\tprint(f\"Eval at step {ind}\")\n",
        "\t\t\tmax_reward, avg_reward, avg_agent_collision, avg_obstacle_collision, avg_border_collision, avg_distance_from_target, avg_on_target, rewards, agent_collisions, obstacle_collisions, border_collisions, distance_from_target, on_target = self.eval(max_reward)\n",
        "\t\t\tself.avg_rewards[ind] = avg_reward\n",
        "\t\t\tself.avg_collisions[\"agent\"][ind] = avg_agent_collision\n",
        "\t\t\tself.avg_collisions[\"obstacle\"][ind] = avg_obstacle_collision\n",
        "\t\t\tself.avg_collisions[\"border\"][ind] = avg_border_collision\n",
        "\t\t\tself.avg_distance_from_target[ind] = avg_distance_from_target\n",
        "\t\t\tself.avg_on_target[ind] = avg_on_target\n",
        "\n",
        "\t\t\tself.rewards[ind] = rewards\n",
        "\t\t\tself.collisions[\"agent\"][ind] = agent_collisions\n",
        "\t\t\tself.collisions[\"obstacle\"][ind] = obstacle_collisions\n",
        "\t\t\tself.collisions[\"border\"][ind] = border_collisions\n",
        "\t\t\tself.distance_from_target[ind] = distance_from_target\n",
        "\t\t\tself.on_target[ind] = on_target\n",
        "\t\tself.trained_for += i\n",
        "\n",
        "\tdef eval(self, max_reward: float|None) -> tuple[float, float, float, float, float, float]:\n",
        "\t\tnum_games = self.eval_settings[\"env_count\"]\n",
        "\t\teval_settings = {k:v for k,v in self.eval_settings.items() if k != \"env_count\"}\n",
        "\t\tenv = self.env_fn(**eval_settings)\n",
        "\n",
        "\t\tmodel = self.model\n",
        "\n",
        "\t\trewards = []\n",
        "\t\tcollisions = {\n",
        "\t\t\t\"agent\": [],\n",
        "\t\t\t\"obstacle\": [],\n",
        "\t\t\t\"border\": []\n",
        "\t\t}\n",
        "\t\tdistance_from_target = []\n",
        "\t\ton_target = []\n",
        "\n",
        "\t\tfor i in range(num_games):\n",
        "\t\t\tenv.reset(seed=i)\n",
        "\n",
        "\t\t\trewards.append([])\n",
        "\t\t\tcollisions[\"agent\"].append([])\n",
        "\t\t\tcollisions[\"obstacle\"].append([])\n",
        "\t\t\tcollisions[\"border\"].append([])\n",
        "\t\t\tdistance_from_target.append([])\n",
        "\t\t\ton_target.append([])\n",
        "\n",
        "\t\t\tfor agent in env.agent_iter():\n",
        "\t\t\t\tobs, reward, termination, truncation, info = env.last()\n",
        "\n",
        "\t\t\t\tif \"render_mode\" in eval_settings and eval_settings[\"render_mode\"] == \"human\":\n",
        "\t\t\t\t\tpygame.event.get()\n",
        "\t\t\t\t\ttime.sleep(0.01 / len(env.possible_agents))\n",
        "\n",
        "\t\t\t\tfor agent in env.agents:\n",
        "\t\t\t\t\trewards[-1].append(env.rewards[agent])\n",
        "\n",
        "\t\t\t\t\tcol = env.unwrapped.world.collision_counter[agent]\n",
        "\t\t\t\t\tcollisions[\"agent\"][-1].append(col[\"agent\"])\n",
        "\t\t\t\t\tcollisions[\"obstacle\"][-1].append(col[\"obstacle\"])\n",
        "\t\t\t\t\tcollisions[\"border\"][-1].append(col[\"border\"])\n",
        "\n",
        "\t\t\t\t\tdistance_from_target[-1].append(env.unwrapped.world.distance_from_target[agent])\n",
        "\t\t\t\t\ton_target[-1].append(int(env.unwrapped.world.distance_from_target[agent] <= 0))\n",
        "\n",
        "\t\t\t\tif termination or truncation:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tact = model.predict(obs, deterministic=True)[0]\n",
        "\n",
        "\t\t\t\tenv.step(act)\n",
        "\n",
        "\t\tenv.close()\n",
        "\n",
        "\t\tavg_reward = np.array(rewards).mean().item()\n",
        "\t\trewards = [sum(r) / len(r) for r in rewards]\n",
        "\n",
        "\t\tagent_collisions = collisions[\"agent\"]\n",
        "\t\tobstacle_collisions = collisions[\"obstacle\"]\n",
        "\t\tborder_collisions = collisions[\"border\"]\n",
        "\t\tavg_agent_collision = np.array(agent_collisions).mean().item()\n",
        "\t\tavg_obstacle_collision = np.array(obstacle_collisions).mean().item()\n",
        "\t\tavg_border_collision = np.array(border_collisions).mean().item()\n",
        "\t\tagent_collisions = [sum(r) / len(r) for r in agent_collisions]\n",
        "\t\tobstacle_collisions = [sum(r) / len(r) for r in obstacle_collisions]\n",
        "\t\tborder_collisions = [sum(r) / len(r) for r in border_collisions]\n",
        "\n",
        "\t\tavg_distance_from_target = np.array(distance_from_target).mean().item()\n",
        "\t\tavg_on_target = np.array(on_target).mean().item()\n",
        "\t\tdistance_from_target = [sum(r) / len(r) for r in distance_from_target]\n",
        "\t\ton_target = [sum(r) / len(r) for r in on_target]\n",
        "\n",
        "\t\tprint(f\"Avg reward: {avg_reward}\")\n",
        "\t\tprint(f\"Avg collisions: agent {avg_agent_collision}, obstacle {avg_obstacle_collision}, border {avg_border_collision}\")\n",
        "\t\tprint(f\"Avg distance from target: {avg_distance_from_target}\")\n",
        "\t\tprint(f\"Avg on target: {avg_on_target}\")\n",
        "\n",
        "\t\tif max_reward is None or avg_reward > max_reward:\n",
        "\t\t\tmax_reward = avg_reward\n",
        "\t\t\tsave_name = f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
        "\t\t\tprint(f'New best reward, saving model with name \"{save_name}\"')\n",
        "\t\t\tmodel.save(save_name)\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"reward {avg_reward} is not larger than {max_reward}\")\n",
        "\n",
        "\t\treturn max_reward, avg_reward, avg_agent_collision, avg_obstacle_collision, avg_border_collision, avg_distance_from_target, avg_on_target, rewards, agent_collisions, obstacle_collisions, border_collisions, distance_from_target, on_target\n",
        "\n",
        "\tdef eval_with_settings(self, settings):\n",
        "\t\tnum_games = settings[\"env_count\"]\n",
        "\t\teval_settings = {k:v for k,v in settings.items() if k != \"env_count\"}\n",
        "\t\tenv = self.env_fn(**eval_settings)\n",
        "\n",
        "\t\tmodel = self.model\n",
        "\n",
        "\t\tfor i in range(num_games):\n",
        "\t\t\tenv.reset(seed=i)\n",
        "\n",
        "\t\t\tfor agent in env.agent_iter():\n",
        "\t\t\t\tobs, reward, termination, truncation, info = env.last()\n",
        "\t\t\t\tprint(agent, \"Obs:\", obs, \"Rew:\", reward)\n",
        "\n",
        "\t\t\t\tif \"render_mode\" in eval_settings and eval_settings[\"render_mode\"] == \"human\":\n",
        "\t\t\t\t\tpygame.event.get()\n",
        "\t\t\t\t\ttime.sleep(0.01 / len(env.possible_agents))\n",
        "\n",
        "\t\t\t\tif termination or truncation:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tact = model.predict(obs, deterministic=True)[0]\n",
        "\n",
        "\t\t\t\tenv.step(act)\n",
        "\n",
        "\t\tenv.close()\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef eval_with_settings_and_model(settings, model, env_fn):\n",
        "\t\tnum_games = settings[\"env_count\"]\n",
        "\t\teval_settings = {k:v for k,v in settings.items() if k != \"env_count\"}\n",
        "\t\tenv = env_fn(**eval_settings)\n",
        "\n",
        "\t\tfor i in range(num_games):\n",
        "\t\t\tenv.reset(seed=i)\n",
        "\n",
        "\t\t\tfor agent in env.agent_iter():\n",
        "\t\t\t\tobs, reward, termination, truncation, info = env.last()\n",
        "\t\t\t\tprint(agent, \"Obs:\", obs, \"Rew:\", reward)\n",
        "\n",
        "\t\t\t\tif \"render_mode\" in eval_settings and eval_settings[\"render_mode\"] == \"human\":\n",
        "\t\t\t\t\tpygame.event.get()\n",
        "\t\t\t\t\ttime.sleep(0.01 / len(env.possible_agents))\n",
        "\n",
        "\t\t\t\tif termination or truncation:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tact = model.predict(obs, deterministic=True)[0]\n",
        "\n",
        "\t\t\t\tenv.step(act)\n",
        "\n",
        "\t\tenv.close()\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef create_env(env_fn, seed: float, cpu_count: int, train_settings: dict):\n",
        "\t\tenv_count = train_settings[\"env_count\"]\n",
        "\t\ttrain_settings = {k:v for k,v in train_settings.items() if k != \"env_count\"}\n",
        "\t\tenv = env_fn(**train_settings)\n",
        "\t\tenv.reset(seed=seed)\n",
        "\t\tenv = pettingzoo_env_to_vec_env_v1(env)\n",
        "\t\tenv = concat_vec_envs_v1(env, env_count, num_cpus=cpu_count, base_class=\"stable_baselines3\")\n",
        "\t\tprint(f\"Created env with settings {(train_settings | {'env_count': env_count}).items()}\")\n",
        "\t\treturn env\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef create_model(name: str, env, n_steps: int, n_epochs: int, batch_size: int, learning_rate: float, ent_coef: float):\n",
        "\t\t#policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=dict(pi=[512]))\n",
        "\t\tmodel = PPO(\n",
        "\t\t\t\"MlpPolicy\",\n",
        "\t\t\tenv,\n",
        "\t\t\tverbose=3,\n",
        "\t\t\tn_steps=n_steps,\n",
        "\t\t\tlearning_rate=0.0001,\n",
        "\t\t\tbatch_size=batch_size,\n",
        "\t\t\tn_epochs=n_epochs,\n",
        "\t\t\tent_coef = ent_coef,\n",
        "\t\t\tdevice=\"cuda\",\n",
        "\t\t\t#policy_kwargs=policy_kwargs\n",
        "\t\t)\n",
        "\t\tprint(f\"Created model {name} with n_steps {n_steps}, n_epochs {n_epochs}, batch_size {batch_size}, learning_rate {learning_rate}, ent_coef {ent_coef}\")\n",
        "\t\treturn model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Xvc61GUCUh"
      },
      "outputs": [],
      "source": [
        "AGENT_COUNT = 16\n",
        "OBSTACLE_COUNT = 8\n",
        "MAX_CYCLES = 512\n",
        "TRAIN_ENV_COUNT = 1\n",
        "TEST_ENV_COUNT = 4\n",
        "BASE_ENV_KWARGS = dict(render_mode=None, N=AGENT_COUNT, O=OBSTACLE_COUNT, local_ratio=1/AGENT_COUNT, max_cycles=MAX_CYCLES, continuous_actions=False, env_count=TRAIN_ENV_COUNT)\n",
        "TRAIN_ENV_KWARGS = BASE_ENV_KWARGS\n",
        "TEST_ENV_KWARGS = BASE_ENV_KWARGS | {\"env_count\": TEST_ENV_COUNT}\n",
        "HUMAN_RENDER_ENV_KWARGS = TEST_ENV_KWARGS | {\"env_count\": 1, \"max_cycles\": 128}\n",
        "ENV_FUNCTION = custom_env\n",
        "\n",
        "TRAIN_N_STEPS = 1024\n",
        "TRAIN_N_EPOCHS = 2\n",
        "TRAIN_BATCH_SIZE = (AGENT_COUNT * TRAIN_ENV_COUNT * TRAIN_N_STEPS) // 2\n",
        "TRAIN_CPU_COUNT = 4\n",
        "TRAIN_PROGRESS_BAR = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA_JAmC4F49u"
      },
      "source": [
        "# 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPfqa1n8UDbh",
        "outputId": "f2581759-6fdf-4aac-c6fd-0eeaa49acaa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model for training\n",
            "Created env with settings dict_items([('render_mode', None), ('N', 16), ('O', 8), ('local_ratio', 0.0625), ('max_cycles', 512), ('continuous_actions', False), ('env_count', 1)])\n",
            "Using cuda device\n",
            "Created model 4_agent with n_steps 1024, n_epochs 2, batch_size 8192, learning_rate 0.0001, ent_coef 0.001\n"
          ]
        }
      ],
      "source": [
        "model_trainer = ModelTrainer(name=\"4_agent\", env_fn=ENV_FUNCTION, seed=None, cpu_count=TRAIN_CPU_COUNT, n_steps=TRAIN_N_STEPS, n_epochs=TRAIN_N_EPOCHS, progress_bar=TRAIN_PROGRESS_BAR, batch_size=TRAIN_BATCH_SIZE, train_settings=TRAIN_ENV_KWARGS, eval_settings=TEST_ENV_KWARGS, learning_rate=0.0001, ent_coef=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7uQr8YCUEq1",
        "outputId": "5d0d196b-40d6-4c47-e66f-68598725bcb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg reward: -0.0045476380657812005\n",
            "Avg collisions: agent 0.2964497131697791, obstacle 0.0005492493592090809, border 0.04593097766385939\n",
            "Avg distance from target: 1.9859378629424413\n",
            "Avg on target: 0.0019528866105211766\n",
            "New best reward, saving model with name \"simple_spread_v3_modified_20231212-214038\"\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 605   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 27    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 604           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 54            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4438738e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.297         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.154         |\n",
            "|    n_updates            | 2             |\n",
            "|    policy_gradient_loss | -1.78e-05     |\n",
            "|    value_loss           | 0.313         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 604           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 81            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4055549e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.352         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.277         |\n",
            "|    n_updates            | 4             |\n",
            "|    policy_gradient_loss | -2.53e-05     |\n",
            "|    value_loss           | 0.551         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 603           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 108           |\n",
            "|    total_timesteps      | 65536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4407233e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.291         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.164         |\n",
            "|    n_updates            | 6             |\n",
            "|    policy_gradient_loss | -3.1e-05      |\n",
            "|    value_loss           | 0.336         |\n",
            "-------------------------------------------\n",
            "Eval at step 4\n",
            "Avg reward: 0.5503621155097086\n",
            "Avg collisions: agent 0.011046014890760405, obstacle 0.009001586720371048, border 0.061252746246796046\n",
            "Avg distance from target: 0.26737092458696793\n",
            "Avg on target: 0.5600951269376296\n",
            "New best reward, saving model with name \"simple_spread_v3_modified_20231212-214327\"\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 604   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 27    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 606           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 54            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5303704e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.468         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.152         |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -5.27e-06     |\n",
            "|    value_loss           | 0.312         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 604          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 81           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.272103e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.45         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.163        |\n",
            "|    n_updates            | 12           |\n",
            "|    policy_gradient_loss | -2.65e-05    |\n",
            "|    value_loss           | 0.332        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 606           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 108           |\n",
            "|    total_timesteps      | 65536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3109966e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.392         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.141         |\n",
            "|    n_updates            | 14            |\n",
            "|    policy_gradient_loss | -1.82e-05     |\n",
            "|    value_loss           | 0.287         |\n",
            "-------------------------------------------\n",
            "Eval at step 8\n",
            "Avg reward: 0.6450077390547027\n",
            "Avg collisions: agent 0.007689491028927133, obstacle 0.0078420602953741, border 0.14322821310875136\n",
            "Avg distance from target: 0.09124086929176296\n",
            "Avg on target: 0.6564426186988893\n",
            "New best reward, saving model with name \"simple_spread_v3_modified_20231212-214614\"\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 628   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 26    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 630          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.020623e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.255        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.141        |\n",
            "|    n_updates            | 18           |\n",
            "|    policy_gradient_loss | -1.86e-05    |\n",
            "|    value_loss           | 0.286        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 630           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 77            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1135126e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.361         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.161         |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -1.83e-05     |\n",
            "|    value_loss           | 0.336         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 630          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.419564e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.326        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.168        |\n",
            "|    n_updates            | 22           |\n",
            "|    policy_gradient_loss | -1.17e-05    |\n",
            "|    value_loss           | 0.338        |\n",
            "------------------------------------------\n",
            "Eval at step 12\n",
            "Avg reward: 0.6673188619946531\n",
            "Avg collisions: agent 0.03533885634077871, obstacle 0.004851702673013548, border 0.1438728182594898\n",
            "Avg distance from target: 0.08630131509665151\n",
            "Avg on target: 0.6801385328939339\n",
            "New best reward, saving model with name \"simple_spread_v3_modified_20231212-214857\"\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 629   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 26    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 628          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 52           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.725851e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.196        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.182        |\n",
            "|    n_updates            | 26           |\n",
            "|    policy_gradient_loss | -9.44e-06    |\n",
            "|    value_loss           | 0.364        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 629           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 78            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.2175604e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.177         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.185         |\n",
            "|    n_updates            | 28            |\n",
            "|    policy_gradient_loss | -1.55e-05     |\n",
            "|    value_loss           | 0.374         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 629           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 104           |\n",
            "|    total_timesteps      | 65536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.2012987e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.148         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.235         |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -7.02e-06     |\n",
            "|    value_loss           | 0.48          |\n",
            "-------------------------------------------\n",
            "Eval at step 16\n",
            "Avg reward: 0.6651425311894451\n",
            "Avg collisions: agent 0.009398266813133162, obstacle 0.008513365067740754, border 0.07855791529354327\n",
            "Avg distance from target: 0.0972542749701073\n",
            "Avg on target: 0.6775601122909801\n",
            "reward 0.6651425311894451 is not larger than 0.6673188619946531\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 630   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 25    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 631           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 51            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9914084e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.0588        |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.727         |\n",
            "|    n_updates            | 34            |\n",
            "|    policy_gradient_loss | -1.09e-05     |\n",
            "|    value_loss           | 1.37          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 632          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.041896e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.00351      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 22.8         |\n",
            "|    n_updates            | 36           |\n",
            "|    policy_gradient_loss | -2.44e-06    |\n",
            "|    value_loss           | 40.9         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 631         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 103         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 2.24416e-07 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | 0.128       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.159       |\n",
            "|    n_updates            | 38          |\n",
            "|    policy_gradient_loss | -2.54e-06   |\n",
            "|    value_loss           | 0.323       |\n",
            "-----------------------------------------\n",
            "Eval at step 20\n",
            "Avg reward: 0.5928804182032804\n",
            "Avg collisions: agent 0.039061546442084705, obstacle 0.006865616990113512, border 0.05948294275601123\n",
            "Avg distance from target: 0.08938009500106536\n",
            "Avg on target: 0.6029346698401075\n",
            "reward 0.5928804182032804 is not larger than 0.6673188619946531\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 635   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 25    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 634          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.905828e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.22         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.18         |\n",
            "|    n_updates            | 42           |\n",
            "|    policy_gradient_loss | -7.3e-06     |\n",
            "|    value_loss           | 0.367        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 632          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.800422e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.0935       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.19         |\n",
            "|    n_updates            | 44           |\n",
            "|    policy_gradient_loss | -1.05e-05    |\n",
            "|    value_loss           | 0.392        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 632         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 103         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 6.62636e-07 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | 0.196       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 0.272       |\n",
            "|    n_updates            | 46          |\n",
            "|    policy_gradient_loss | -1.34e-05   |\n",
            "|    value_loss           | 0.56        |\n",
            "-----------------------------------------\n",
            "Eval at step 24\n",
            "Avg reward: 0.6021478307872324\n",
            "Avg collisions: agent 0.006713047723666545, obstacle 0.007964115708531673, border 0.06696074392774319\n",
            "Avg distance from target: 0.10001618740191419\n",
            "Avg on target: 0.6118790430855608\n",
            "reward 0.6021478307872324 is not larger than 0.6673188619946531\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 634   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 25    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 634           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 51            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0499534e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.000394      |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 72.5          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -8.21e-06     |\n",
            "|    value_loss           | 143           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 632          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.361276e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | -0.0767      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 0.224        |\n",
            "|    n_updates            | 52           |\n",
            "|    policy_gradient_loss | -1.35e-05    |\n",
            "|    value_loss           | 0.452        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 631          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.307746e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 1.73e-05     |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 73.1         |\n",
            "|    n_updates            | 54           |\n",
            "|    policy_gradient_loss | -5.57e-06    |\n",
            "|    value_loss           | 150          |\n",
            "------------------------------------------\n",
            "Eval at step 28\n",
            "Avg reward: 0.6150049586850845\n",
            "Avg collisions: agent 0.011107042597339191, obstacle 0.014768704992066398, border 0.1600165537654095\n",
            "Avg distance from target: 0.10426230240381726\n",
            "Avg on target: 0.6253566306603198\n",
            "reward 0.6150049586850845 is not larger than 0.6673188619946531\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 631   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 25    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 629           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 52            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9402585e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | -0.0608       |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.516         |\n",
            "|    n_updates            | 58            |\n",
            "|    policy_gradient_loss | -9.97e-06     |\n",
            "|    value_loss           | 1.05          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 630          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.998939e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.00347      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 76.9         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -8.87e-06    |\n",
            "|    value_loss           | 144          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 631          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.799061e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.61        |\n",
            "|    explained_variance   | 0.00107      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 16           |\n",
            "|    n_updates            | 62           |\n",
            "|    policy_gradient_loss | -8.4e-06     |\n",
            "|    value_loss           | 37.6         |\n",
            "------------------------------------------\n",
            "Eval at step 32\n",
            "Avg reward: 0.6063391675728885\n",
            "Avg collisions: agent 0.03747482607103625, obstacle 0.008421823507872574, border 0.06339062309288417\n",
            "Avg distance from target: 0.10995171149010934\n",
            "Avg on target: 0.617346744171854\n",
            "reward 0.6063391675728885 is not larger than 0.6673188619946531\n"
          ]
        }
      ],
      "source": [
        "model_trainer.learn(update_count=32, eval_after=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViCQmp93aMa7",
        "outputId": "c5bfcfbc-7105-4f7b-8862-e5a7c1f69f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: -0.0045476380657812005, 4: 0.5503621155097086, 8: 0.6450077390547027, 12: 0.6673188619946531, 16: 0.6651425311894451, 20: 0.5928804182032804, 24: 0.6021478307872324, 28: 0.6150049586850845, 32: 0.6063391675728885}\n",
            "{0: 1.9859378629424413, 4: 0.26737092458696793, 8: 0.09124086929176296, 12: 0.08630131509665151, 16: 0.0972542749701073, 20: 0.08938009500106536, 24: 0.10001618740191419, 28: 0.10426230240381726, 32: 0.10995171149010934}\n",
            "{0: 0.0019528866105211766, 4: 0.5600951269376296, 8: 0.6564426186988893, 12: 0.6801385328939339, 16: 0.6775601122909801, 20: 0.6029346698401075, 24: 0.6118790430855608, 28: 0.6253566306603198, 32: 0.617346744171854}\n",
            "{0: 0.2964497131697791, 4: 0.011046014890760405, 8: 0.007689491028927133, 12: 0.03533885634077871, 16: 0.009398266813133162, 20: 0.039061546442084705, 24: 0.006713047723666545, 28: 0.011107042597339191, 32: 0.03747482607103625}\n",
            "{0: 0.0005492493592090809, 4: 0.009001586720371048, 8: 0.0078420602953741, 12: 0.004851702673013548, 16: 0.008513365067740754, 20: 0.006865616990113512, 24: 0.007964115708531673, 28: 0.014768704992066398, 32: 0.008421823507872574}\n",
            "{0: 0.2964497131697791, 4: 0.011046014890760405, 8: 0.007689491028927133, 12: 0.03533885634077871, 16: 0.009398266813133162, 20: 0.039061546442084705, 24: 0.006713047723666545, 28: 0.011107042597339191, 32: 0.03747482607103625}\n"
          ]
        }
      ],
      "source": [
        "print(model_trainer.avg_rewards)\n",
        "print(model_trainer.avg_distance_from_target)\n",
        "print(model_trainer.avg_on_target)\n",
        "print(model_trainer.avg_collisions[\"agent\"])\n",
        "print(model_trainer.avg_collisions[\"obstacle\"])\n",
        "print(model_trainer.avg_collisions[\"agent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Z9P6nHyCWC"
      },
      "source": [
        "# 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDCCQP1-yBjs",
        "outputId": "4c862be0-d0cf-4a74-955b-c5fbea700cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model for training\n",
            "Created env with settings dict_items([('render_mode', None), ('N', 16), ('O', 8), ('local_ratio', 0.0625), ('max_cycles', 512), ('continuous_actions', False), ('env_count', 1)])\n",
            "Using cuda device\n",
            "Created model action_and_value with n_steps 1024, n_epochs 2, batch_size 8192, learning_rate 0.0001, ent_coef 0.001\n"
          ]
        }
      ],
      "source": [
        "model_trainer = ModelTrainer(name=\"action_and_value\", env_fn=ENV_FUNCTION, seed=None, cpu_count=TRAIN_CPU_COUNT, n_steps=TRAIN_N_STEPS, n_epochs=TRAIN_N_EPOCHS, progress_bar=TRAIN_PROGRESS_BAR, batch_size=TRAIN_BATCH_SIZE, train_settings=TRAIN_ENV_KWARGS, eval_settings=TEST_ENV_KWARGS, learning_rate=0.0001, ent_coef=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IHwnW1ViyFFy",
        "outputId": "e4f9aca4-92c9-4f1a-b9f1-47c6fa03fb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg reward: 0.03590755220634628\n",
            "Avg collisions: agent 0.0671381056999878, obstacle 0.00387525936775296, border 0.011902309898694008\n",
            "Avg distance from target: 1.2302655905221225\n",
            "Avg on target: 0.03567450872696204\n",
            "New best reward, saving model with name \"simple_spread_v3_modified_20231212-220329\"\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 618   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 26    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 621           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 52            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1225602e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.00959       |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 18.9          |\n",
            "|    n_updates            | 2             |\n",
            "|    policy_gradient_loss | -9.48e-06     |\n",
            "|    value_loss           | 37            |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 624           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 78            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1400298e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.61         |\n",
            "|    explained_variance   | 0.117         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 0.178         |\n",
            "|    n_updates            | 4             |\n",
            "|    policy_gradient_loss | -2.62e-05     |\n",
            "|    value_loss           | 0.365         |\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-cd59cc6e3cbe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_after\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-064d2d18ec71>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, update_count, eval_after)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mupdate_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps_per_update\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meval_after\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meval_after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_for\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 315\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supersuit/vector/sb3_vector_wrapper.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Note: SB3 expects dones to be an np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         dones = np.array(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supersuit/vector/concat_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supersuit/vector/concat_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvenv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             data.append(\n\u001b[0;32m---> 84\u001b[0;31m                 venv.step(\n\u001b[0m\u001b[1;32m     85\u001b[0m                     self.concatenate_actions(\n\u001b[1;32m     86\u001b[0m                         \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supersuit/vector/markov_vector_wrapper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         }\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpar_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# adds last observation to info where user can get it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    204\u001b[0m                         \u001b[0;34mf\"expected agent {agent} got agent {self.aec_env.agent_selection}, Parallel environment wrapper expects agents to step in a cycle.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                     )\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/env.py\u001b[0m in \u001b[0;36mlast\u001b[0;34m(self, observe)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mobserve\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         return (\n\u001b[1;32m    188\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mEnvLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_observe_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgentID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mObsType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgentID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mObsType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         return self.scenario.observation(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         ).astype(np.float32)\n",
            "\u001b[0;32m<ipython-input-17-99c2c74357b5>\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, agent, world)\u001b[0m\n\u001b[1;32m    460\u001b[0m                         \u001b[0mrelative_position_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                         \u001b[0mrelative_position_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_position\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_collision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                                 \u001b[0mrelative_position_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                                 \u001b[0mrelative_position_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-99c2c74357b5>\u001b[0m in \u001b[0;36mis_collision\u001b[0;34m(self, agent1, agent2)\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance_from_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mis_collision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                 \u001b[0mdelta_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_trainer.learn(update_count=32, eval_after=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taexSQnzyF4T"
      },
      "outputs": [],
      "source": [
        "print(model_trainer.avg_rewards)\n",
        "print(model_trainer.avg_distance_from_target)\n",
        "print(model_trainer.avg_on_target)\n",
        "print(model_trainer.avg_collisions[\"agent\"])\n",
        "print(model_trainer.avg_collisions[\"obstacle\"])\n",
        "print(model_trainer.avg_collisions[\"agent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWoYdzemyGla"
      },
      "source": [
        "# 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WstqhI1yHUW"
      },
      "outputs": [],
      "source": [
        "model_trainer = ModelTrainer(name=\"action_and_value\", env_fn=ENV_FUNCTION, seed=None, cpu_count=TRAIN_CPU_COUNT, n_steps=TRAIN_N_STEPS, n_epochs=TRAIN_N_EPOCHS, progress_bar=TRAIN_PROGRESS_BAR, batch_size=TRAIN_BATCH_SIZE, train_settings=TRAIN_ENV_KWARGS, eval_settings=TEST_ENV_KWARGS, learning_rate=0.0001, ent_coef=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1WLlB_3yIV_"
      },
      "outputs": [],
      "source": [
        "model_trainer.learn(update_count=32, eval_after=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8b0zk0xyIl7"
      },
      "outputs": [],
      "source": [
        "print(model_trainer.avg_rewards)\n",
        "print(model_trainer.avg_distance_from_target)\n",
        "print(model_trainer.avg_on_target)\n",
        "print(model_trainer.avg_collisions[\"agent\"])\n",
        "print(model_trainer.avg_collisions[\"obstacle\"])\n",
        "print(model_trainer.avg_collisions[\"agent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOsyvn3ryZYi"
      },
      "source": [
        "# 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpWB8tSUyd8k"
      },
      "outputs": [],
      "source": [
        "model_trainer = ModelTrainer(name=\"action_and_value\", env_fn=ENV_FUNCTION, seed=None, cpu_count=TRAIN_CPU_COUNT, n_steps=TRAIN_N_STEPS, n_epochs=TRAIN_N_EPOCHS, progress_bar=TRAIN_PROGRESS_BAR, batch_size=TRAIN_BATCH_SIZE, train_settings=TRAIN_ENV_KWARGS, eval_settings=TEST_ENV_KWARGS, learning_rate=0.0001, ent_coef=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoV5QwdIyeL-"
      },
      "outputs": [],
      "source": [
        "model_trainer.learn(update_count=32, eval_after=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSE-AmzlyecQ"
      },
      "outputs": [],
      "source": [
        "print(model_trainer.avg_rewards)\n",
        "print(model_trainer.avg_distance_from_target)\n",
        "print(model_trainer.avg_on_target)\n",
        "print(model_trainer.avg_collisions[\"agent\"])\n",
        "print(model_trainer.avg_collisions[\"obstacle\"])\n",
        "print(model_trainer.avg_collisions[\"agent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTeFr3t6yaiw"
      },
      "source": [
        "# 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl0UZr6yyev0"
      },
      "outputs": [],
      "source": [
        "model_trainer = ModelTrainer(name=\"action_and_value\", env_fn=ENV_FUNCTION, seed=None, cpu_count=TRAIN_CPU_COUNT, n_steps=TRAIN_N_STEPS, n_epochs=TRAIN_N_EPOCHS, progress_bar=TRAIN_PROGRESS_BAR, batch_size=TRAIN_BATCH_SIZE, train_settings=TRAIN_ENV_KWARGS, eval_settings=TEST_ENV_KWARGS, learning_rate=0.0001, ent_coef=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKhPf4t5ye7R"
      },
      "outputs": [],
      "source": [
        "model_trainer.learn(update_count=32, eval_after=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNblIz50yfPM"
      },
      "outputs": [],
      "source": [
        "print(model_trainer.avg_rewards)\n",
        "print(model_trainer.avg_distance_from_target)\n",
        "print(model_trainer.avg_on_target)\n",
        "print(model_trainer.avg_collisions[\"agent\"])\n",
        "print(model_trainer.avg_collisions[\"obstacle\"])\n",
        "print(model_trainer.avg_collisions[\"agent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDTj6L9GybHi"
      },
      "source": [
        "# 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e58SRxhmyThG"
      },
      "outputs": [],
      "source": [
        "model_trainer = ModelTrainer(name=\"action_and_value\", env_fn=ENV_FUNCTION, seed=None, cpu_count=TRAIN_CPU_COUNT, n_steps=TRAIN_N_STEPS, n_epochs=TRAIN_N_EPOCHS, progress_bar=TRAIN_PROGRESS_BAR, batch_size=TRAIN_BATCH_SIZE, train_settings=TRAIN_ENV_KWARGS, eval_settings=TEST_ENV_KWARGS, learning_rate=0.0001, ent_coef=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN3YlFtQyfn1"
      },
      "outputs": [],
      "source": [
        "model_trainer.learn(update_count=32, eval_after=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-oRrPvhyf54"
      },
      "outputs": [],
      "source": [
        "print(model_trainer.avg_rewards)\n",
        "print(model_trainer.avg_distance_from_target)\n",
        "print(model_trainer.avg_on_target)\n",
        "print(model_trainer.avg_collisions[\"agent\"])\n",
        "print(model_trainer.avg_collisions[\"obstacle\"])\n",
        "print(model_trainer.avg_collisions[\"agent\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHR41zdZygKz"
      },
      "outputs": [],
      "source": [
        "print(\"asd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap4QXjAl7RjA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(time.time())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvAVbrAYt3K0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}